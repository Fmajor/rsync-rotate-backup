#!/bin/env python
import os
import time
import subprocess
import MySQLdb
import yaml
import os
import random
import sys
import argparse
import glob
import humanize
import fnmatch
import copy
import calendar
import json
from pprint import pprint
from prettytable import PrettyTable
from datetime import datetime, timedelta
from dateutil.relativedelta import relativedelta
from dateutil.parser import parser as time_parser

## config and setup
DEFAULT_CONFIG_FILE = 'mysql-rotate-backup.yml'
DESCRIPTION='''Backup database manually or in rotation mode.'''

TIMEFORMAT = "%Y-%m-%dT%H:%M:%S"

DEFAULT_CONFIG='''## mysql-rotate-backup config file
output: mysql-rotate-backup
host: localhost
port: 3306
user: root
password: ''
databases:
tables:
interval: 10m # min interval of two auto rotate backup
max-age: 2y # max age of one auto rotate backup

# how long will backup stay at xx level
max-year: 5
max-month: 12
max-week: 6
max-day: 7
max-hour: 24
max-minute: 60

# the interval in xx level, if 0, do not backup at this level, should less than max-xx
interval-year: 1
interval-month: 1
interval-week: 1
interval-day: 1
interval-hour: 3
interval-minute: 30


# the start of each xx
start-month:     1 # 1~12
start-day-month: 1 # 1~28, can be negitage
start-day-week:  1 # 1~7
start-hour:      0 # 0~59
start-minute:    0 # 0~59
start-second:    0 # 0~59

## restore configs
binlog_dir: /var/lib/mysql
binlog_prefix: mariadb-bin
restore_host: localhost
restore_port: 3306
restore_user: root
restore_password: ''
restore_databases:
restore_tables:
stopBefore:
startBefore:

## use mysql-rotate-backup > mysql-rotate-backup.yml to generate the default config file'''

# default config
c = config = yaml.safe_load(DEFAULT_CONFIG)
for key in list(config.keys()):
  if config.get(key) is None:
    config.pop(key)

parser = argparse.ArgumentParser(
  description=DESCRIPTION,
  formatter_class=argparse.RawTextHelpFormatter,
)
parser.add_argument('command',          type=str, help='''status   show the current backupfile
backup       do backup
restore      generate restore scripts
config       print current config details
init-config  get init config file
''', nargs='?')
parser.add_argument('-c', '--config',    type=str, help='config file path')
# backup configs
parser.add_argument('--host',            type=str, help='database hostname (default: localhost)')
parser.add_argument('-P', '--port',      type=str, help='database port (default: 3306)')
parser.add_argument('-u', '--user',      type=str, help='database username (default root)')
parser.add_argument('-p', '--password',  type=str, help='database password')
parser.add_argument('-d', '--databases', type=str, help='databases to backup')
parser.add_argument('-t', '--tables',    type=str, help='database tables to backup (must also have database)')
parser.add_argument('-i', '--interval',  type=str, help='minimal backup interval, examples: 10m, 1h, 12h, 1d, 1M, 1y')
parser.add_argument('-o', '--output',    type=str, help='output directory (default: mysql-rotate-backup)')
parser.add_argument('-n', '--name', metavar='NAME', type=str, help='''do one manual backup with given NAME.
If NAME is empty, use timestamp as the NAME.''')
# restore configs
parser.add_argument('--stopBefore',  metavar="TIME", type=time_parser, help='will find the binlog to restore to this iso time (default: now)')
parser.add_argument('--startBefore', metavar="TIME", type=time_parser, help='will use the newest backup earlyer than this iso time (default: now)')
parser.add_argument('--restore_host', metavar="HOST", type=str, help='restore database hostname (default: localhost)')
parser.add_argument('--restore_port', metavar="PORT", type=str, help='restore database port (default: 3306)')
parser.add_argument('--restore_user', metavar="USER", type=str, help='restore database username (default root)')
parser.add_argument('--restore_password', metavar="PASSWORD", type=str, help='restore database password')
parser.add_argument('--restore_databases', metavar="DATABASES",type=str, help='databases to restore')
parser.add_argument('--restore_tables', metavar="TABLES", type=str, help='database tables to restore (must also have restore_database)')

def check_config(config):
  if config.get('tables'):
    if config.get('databases') is None:
      print('if you set `table`, you must also set `databases`')
      sys.exit(1)
    elif len(config.get('databases').split()) > 1:
      print('if you set `table`, you must have no more than one `databases`')
      sys.exit(1)
  if config.get('restore_tables'):
    if config.get('restore_databases') is None:
      print('if you set `restore_table`, you must also set `restore_databases`')
      sys.exit(1)
    elif len(config.get('databases').split()) > 1:
      print('if you set `restore_table`, you must have no more than one `restore_databases`')
      sys.exit(1)
def process_time(s):
  'e.g., 1h => 1h deltatime'
  unit = s[-1]
  value = float(s[:-1])
  if unit not in ['s', 'm', 'h', 'd', 'M', 'y', 'w']:
    raise Exception('time interval must have the write unit in smhdMyw, yours are {}'.format(s))
  if unit == 's':
    return relativedelta(seconds=value)
  elif unit == 'm':
    return relativedelta(minutes=value)
  elif unit == 'h':
    return relativedelta(hours=value)
  elif unit == 'd':
    return relativedelta(days=value)
  elif unit == 'M':
    return relativedelta(months=value)
  elif unit == 'y':
    return relativedelta(years=value)
  elif unit == 'w':
    return relativedelta(weeks=value)
def delete_by_max_age(to_reserve, to_delete, now=None, config=None):
  this_to_reserve = []
  this_to_delete = []
  oldest_time = now - process_time(config['max-age'])
  for eachfile in to_reserve:
    if eachfile['timestamp'] <= oldest_time:
      eachfile['reason'] = 'older than {}'.format(config['max-age'])
      this_to_delete.append(eachfile)
    else:
      this_to_reserve.append(eachfile)
  return this_to_reserve, this_to_delete
def delete_by_min_interval(to_reserve, to_delete, now=None, config=None):
  this_to_reserve = []
  min_delta = process_time(config['interval'])
  if len(to_reserve) <= 1:
    return to_reserve, to_delete
  this_to_reserve.append(to_reserve[0])
  thisindex = 0
  nextindex = 1
  while nextindex < len(to_reserve):
    thisfile = to_reserve[thisindex]
    nextfile = to_reserve[nextindex]
    if thisfile['timestamp'] - min_delta < nextfile['timestamp']: # should delete nextfile
      # print(thisfile['timestampStr'], nextfile['timestampStr'], min_delta)
      nextfile['reason'] = 'interval: {}, too near with {}'.format(config['interval'], thisfile['timestampStr'])
      to_delete.append(nextfile)
      nextindex += 1
    else: # should reserve nextfile and set it to thisindex
      this_to_reserve.append(nextfile)
      thisindex = nextindex
      nextindex += 1
  return this_to_reserve, to_delete
unitNameMap = {
  'minute': 'm',
  'hour': 'h',
  'day': 'd',
  'week': 'w',
  'month': 'M',
  'year': 'y',
}
def delete_in_levels(to_reserve, to_delete, config=None):
  this_to_reserve = copy.deepcopy(to_reserve)
  lg = last_good_file = to_reserve[0]['timestamp']
  for level in ['minute', 'hour', 'day', 'week', 'month', 'year']:
    max_length = str(config.get('max-{}'.format(level), 0))
    interval   = str(config.get('interval-{}'.format(level), 0))
    if max_length==0 or interval ==0:
      continue
    max_length = process_time(str(max_length) + unitNameMap[level])
    interval = process_time(str(interval) + unitNameMap[level])
    if level == 'minute':
      true_start = datetime(
        year=lg.year, month=lg.month, day=lg.day, hour=lg.hour,
        minute=lg.minute, second=config.get('start-second')
      )
    elif level == 'hour':
      true_start = datetime(
        year=lg.year, month=lg.month, day=lg.day, hour=lg.hour,
        minute=config.get('start-minute'), second=config.get('start-second')
      )
    elif level == 'day':
      true_start = datetime(
        year=lg.year, month=lg.month, day=lg.day, hour=config.get('start-hour'),
        minute=config.get('start-minute'), second=config.get('start-second')
      )
    elif level == 'week':
      onDay = lambda date, day: date + timedelta(days=(day-date.weekday()+7)%7) - timedelta(days=7)
      true_start = datetime(
        year=lg.year, month=lg.month,
        day=lg.day, hour=config.get('start-hour'),
        minute=config.get('start-minute'), second=config.get('start-second')
      )
      true_start = onDay(true_start, config.get('start-weekday',1) - 1)
    elif level == 'month':
      day = config.get('start-day-month', 0)
      if day < 0:
        day = calendar.monthrange(lg.year, lg.month)[1] + day
      true_start = datetime(
        year=lg.year, month=lg.month,
        day=day, hour=config.get('start-hour'),
        minute=config.get('start-minute'), second=config.get('start-second')
      )
    elif level == 'year':
      true_start = datetime(
        year=lg.year, month=config.get('start-month'),
        day=config.get('start-day-month'), hour=config.get('start-hour'),
        minute=config.get('start-minute'), second=config.get('start-second')
      )

    #  |one interval|one interval|one interval|one interval|....|one interval|
    #           lg [true_start,  o,           o,           o,...o,    true_end, max_end], other...
    while lg < true_start:
      true_start = true_start - interval
    max_end = true_start - max_length
    end = true_start
    intervals = [(true_start, lg)]
    # true start is the latest
    while end - interval >= max_end:
      right = end
      end = end - interval
      left = end
      true_end = end # the oldest
      intervals.append((left, right))
    # not processed in this level
    this_level_to_reserve = list(filter(lambda _:_['timestamp']>lg or _['timestamp']<=true_end, this_to_reserve))
    to_process = list(filter(lambda _:_['timestamp']<=lg and _['timestamp']>true_end, this_to_reserve))
    ##print('==========================================')
    ##print(lg)
    ##print(true_start, true_end, interval)
    ##print('  to_process_length: {}, reserve_length:{}, total_length:{}'.format(len(to_process), len(this_level_to_reserve), len(this_to_reserve)))
    processed_reserve = []
    # in this_level_to_process
    for left, right in intervals:
      # in each interval, only have one
      this_files = list(filter(lambda _:_['timestamp']>left and _['timestamp']<=right, to_process))
      if len(this_files):
        this_files.sort(key=lambda _:_['timestamp'])
        r = this_files[-1]
        d = this_files[:-1]
        processed_reserve.append(r)
        for each in d:
          each['reason'] = 'level: {}, {} ~ {} only use {}'.format(
            level,
            left.strftime(TIMEFORMAT),
            right.strftime(TIMEFORMAT),
            r['timestamp'].strftime(TIMEFORMAT),
          )
          to_delete.append(each)
    this_level_to_reserve.extend(processed_reserve)
    if len(processed_reserve):
      processed_reserve.sort(key = lambda _:_['timestamp'])
      lg = processed_reserve[0]['timestamp']
    ##for each in reversed(processed_reserve): print(each['timestampStr'])
    ##print('  processed_reverse_length: {}, total_length: {}'.format(len(processed_reserve), len(this_level_to_reserve)))
    this_to_reserve = this_level_to_reserve
  this_to_reserve.sort(key=lambda _:_['timestamp'])
  this_to_reserve.reverse()
  to_delete.sort(key=lambda _:_['timestamp'])
  to_delete.reverse()
  return this_to_reserve, to_delete
def get_clean_list(output, config=None, now=None):
  ## get old backups
  autoclean = {}
  rootpath = os.path.join(output, 'backup', 'auto')
  for root, dirs, files in os.walk(rootpath):
    relpath = os.path.relpath(root, rootpath)
    if len(files):
      fileinfo = []
      for eachfile in files:
        if not eachfile.endswith('.gz'):
          continue
        timestampStr = eachfile.split('|')[-1].split('_')[0]
        timestamp = datetime.strptime(timestampStr, TIMEFORMAT)
        fileinfo.append({
          'filename': os.path.join(root, eachfile),
          'timestampStr': timestampStr,
          'timestamp': timestamp,
        })

      autoclean[relpath] = {
        'raw': fileinfo,
        'to_delete': None,
        'to_reserve': None,
      }
  for eachpath in autoclean:
    todo = autoclean[eachpath]
    raw = todo['raw']
    raw.sort(key=lambda _:_['timestamp'])
    raw.reverse()
    to_delete = []
    to_reserve = copy.deepcopy(raw)
    to_reserve, to_delete = delete_by_max_age(to_reserve, to_delete, now=now, config=config)
    to_reserve, to_delete = delete_by_min_interval(to_reserve, to_delete, now=now, config=config)
    to_reserve, to_delete = delete_in_levels(to_reserve, to_delete, config=config)
    autoclean[eachpath]['to_delete'] = to_delete
    autoclean[eachpath]['to_reserve'] = to_reserve
  return autoclean

def do_backup(config=None, now=None, logfile=None):
  timestamp = now.strftime(TIMEFORMAT)
  print(timestamp)
  print_config(config)
  try: ## test connection and get latest binlog
    conn= MySQLdb.connect(
      host=config['host'],
      port=config['port'],
      user=config['user'],
      passwd=config['password'],
    )
    cur = conn.cursor()
  except Exception as e:
    print(e)
    sys.exit(1)
  cur.execute('show master status')
  binlog_name = cur.fetchall()
  if len(binlog_name):
    binlog_name = binlog_name[0][0]
    binlog_pre = '.'.join(binlog_name.split('.')[:-1])
    binlog_index = binlog_name.split('.')[-1]
    next_binlog_index = '.{:06d}'.format(int(binlog_index) + 1)
    next_binlog_name = binlog_pre + next_binlog_index
  else:
    next_binlog_name = 'nobinlog'

  ## read name, databases and tables, decide backup object
  backup_login = '-u{user} -p{password}'.format(
    user=c['user'],
    password=c['password'],
  )
  backup_args = '-h{host} -P{port} --add-drop-table --add-drop-database -F -R'.format(
    host=c['host'],
    port=c['port'],
  )
  if config.get('name') is not None:
    output_path = os.path.join(output, 'backup', 'manual', config.get('name'))
    if config.get('databases') is not None:
      if config.get('tables') is not None:
        # have name, databases and tables
        backup_obj = '--tables {} {}'.format(c['databases'], c['tables'])
        prefix = "databases:{}&tables:{}|".format(c['databases'], c['tables'])
      else:
        # have name and databases
        backup_obj = '--databases {}'.format(c['databases'])
        prefix = "databases:{}|".format(c['databases'])
    else:
      # have name
      backup_obj = '--all-databases'
      prefix = "full|"
  else:
    if config.get('databases') is not None:
      if config.get('tables') is not None:
        # databases and tables
        backup_obj = '--tables {} {}'.format(c['databases'], c['tables'])
        output_path = os.path.join(output, 'backup', 'auto', c['databases'], c['tables'])
        prefix = "databases:{}&tables:{}|".format(c['databases'], c['tables'])
      else:
        # databases
        backup_obj = '--databases {}'.format(c['databases'])
        output_path = os.path.join(output, 'backup', 'auto', c['databases'])
        prefix = "databases:{}|".format(c['databases'])
    else:
      # full
      backup_obj = '--all-databases'
      output_path = os.path.join(output, 'backup', 'auto')
      prefix = "full|"
  os.makedirs(output_path, exist_ok=True)

  filename = os.path.join(output_path, "{prefix}{timestamp}_{next_binlog_name}.gz".format(
      prefix=prefix, timestamp=timestamp, next_binlog_name=next_binlog_name
    )
  )

  cmd = '''mysqldump {backup_login} {backup_args} {backup_obj} | gzip > "{filename}"'''.format(**locals())
  pcmd = '''mysqldump {backup_args} {backup_obj} | gzip > "{filename}"'''.format(**locals())
  print(pcmd)
  time0 = time.time()
  print('start to backup with: {backup_obj}'.format(**locals()))
  ps = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
  stdout = ps.stdout.read().decode()
  stderr = ps.stderr.read().decode()
  if stdout or stderr:
    print('stdout:', stdout)
    print('stderr:', stderr)
    print('have error!')
    if os.path.exists(filename):
      os.remove(filename)
      print('delete file:\n  ', filename)
      dirname = os.path.dirname(filename)
      if not os.listdir(dirname):
        os.rmdir(dirname)
        print('delete empty folder:\n  ', dirname)
    sys.exit(1)
  else:
    print('finish backup in {:.1f}s'.format(time.time() - time0))
  print('=======log=======')
  ## clean old backups
  cleanList = get_clean_list(output, config=config, now=now)

  ## write to log
  LOG_TEMPLAGE="{timestamp} {command} "
  log = LOG_TEMPLAGE.format(
    timestamp=timestamp,
    command=config['command'],
  ) + "{name} {filename}".format(
    filename=os.path.basename(filename),
    name='name:'+c.get('name') if c.get('name') else 'auto',
  )
  logs = [log]
  for eachdir in cleanList:
    if len(cleanList[eachdir]['to_delete']):
      logs.append('  clean {}'.format(eachdir))
      for eachfile in cleanList[eachdir]['to_delete']:
        os.remove(eachfile['filename'])
        logs.append('    delete '+eachfile['filename'])
        logs.append('      '+eachfile['reason'])

  log = '\n'.join(logs)
  if os.path.exists(logfile):
    with open(logfile, 'a') as f:
      f.write('\n' + log)
  else:
    with open(logfile, 'a') as f:
      f.write(log)
  print(log)
def find_files(directory, pattern):
  for root, dirs, files in os.walk(directory):
    for basename in files:
      if fnmatch.fnmatch(basename, pattern):
        filename = os.path.join(root, basename)
        yield filename
def do_status(config=None, now=None, logfile=None):
  output = config['output']
  timestamp = now.strftime(TIMEFORMAT)
  pathStr = 'backup path: {}'.format(os.path.abspath(output))
  print(pathStr)
  print('-' * len(pathStr))
  for name in ['manual', 'auto']:
    backup_dir = os.path.join(output, 'backup', name)
    backups_raw = []
    for filename in find_files(backup_dir, '*.gz'):
      backups_raw.append(filename)
    backups_raw.sort(key=lambda _:_.split('|')[-1])
    if len(backups_raw):
      if name=='auto':
        backups = [[os.path.basename(_), humanize.naturalsize(os.path.getsize(_))] for _ in backups_raw]
        table = PrettyTable(['file', 'size'])
        table.align["file"] = "r"
        for row in backups:
          table.add_row(row)
      else:
        backups = [[os.path.basename(_), humanize.naturalsize(os.path.getsize(_)), os.path.basename(os.path.dirname(_))] for _ in backups_raw]
        table = PrettyTable(['file', 'size', 'name'])
        table.align["file"] = "r"
        for row in backups:
          table.add_row(row)

      print('{} backup'.format(name))
      print(table)
    else:
      print('{} backup'.format(name))
      print('  empty')

# string, list or none, string, list or none
def backup_match(db, _db, tb, _tb):
  'db and tb are requests, _db and _tb are actual content for this file'
  if db is None and tb is None: # request full
    if _db is None and _tb is None:
      return True
    else:
      return False
  elif db: # request
    if _db is None: # full backup, good
      return True
    dbs = db.split()
    _dbs = _db
    for request in dbs: # must contain this db
      if request not in _dbs:
        return False
    # database contains
    if _tb is None and tb is not None: # backup have full database, request tables from it
      return True
    elif _tb is not None and tb is None: # backup have several tables, request all tables from it
      return False
    elif _tb is None and tb is None: # backup have full database, request full
      return True
    else: # both request db and tb
      tbs = tb.split()
      _tbs = _tb
      for request in tbs: # must contain this db
        if request not in _tbs:
          return False
      # table contains
      return True
  else: # not db and tb, should not be this
    return False

def check_binlog(names):
  names.sort()
  if len(names) == 0:
    return []
  elif len(names) == 1:
    return [names[0], names[0]]
  for index in range(1, len(names)-1):
    thisname = names[index-1]
    nextname = names[index]
    try:
      thisindex = int(thisname.split('.')[-1])
      nextindex = int(nextname.split('.')[-1])
      assert thisindex+1 == nextindex
    except:
      raise Exception( 'bad binlog sequence: {} => {}'.format(thisname, nextname))
  return [names[0], names[-1]]
def gen_binlog_info(binlogs, name=None):
  names = list(binlogs.keys())
  names.sort()
  output = [
    '# restore task: {}'.format(name),
    '# binlog information',
  ]
  for name in names:
    ctime = binlogs[name]['ctime'].strftime(TIMEFORMAT)
    mtime = binlogs[name]['mtime'].strftime(TIMEFORMAT)
    size  = binlogs[name]['size']
    output.append('{} {} {} {}'.format(name, ctime, mtime, size))
  return '\n'.join(output)
def gen_backup_info(backups, config, name=None):
  output = [
    '# restore task: {}'.format(name),
    '# backup file that match restore parameters',
    '# startBefore = {}'.format(config.get('startBefore')),
    '# stopBefore  = {}'.format(config.get('stopBefore')),
    '# databases   = {}'.format(config.get('databases')),
    '# tables      = {}'.format(config.get('tables')),
  ]
  index = 0
  for type in backups:
    if type == 'manual':
      output.append('manual backups:')
    else:
      output.append('auto backups:')
    table = PrettyTable(['ID', 'timestamp', 'full', 'databases', 'tables', 'binlog', 'errors'])
    #table.align["file"] = "r"
    for each in backups[type]:
      index += 1
      each['index'] = index
      table.add_row((
        index,
        each['timestampStr'],
        not (each['databases'] or each['tables'] ),
        each['databases'] if each['databases'] else '',
        each['tables'] if each['tables'] else '',
        each['binlog'],
        each['errors'],
      ))
    output.append(str(table))
  return '\n'.join(output)



RESTORE_TEMPLATE='''
## Do restores, you should read this scripts and modify these commands when needed
# 1. do a full restore

restore_host={restore_host}
restore_port={restore_port}
restore_user={restore_user}
restore_password={restore_password}
backup_file="../../../{relpath}"

echo "do full restore from ${{backup_file}}"
{db_restore_string}

# 2. replay binlog
echo "replay binlog range: {binlog_range}"
binlog_dir={binlog_dir}

{binlog_restore_string}
echo "job done"
'''
# not do format
FULL_RESTORE_TEMPLATE='''  mysql -h${restore_host} -P${restore_port} -u${restore_user} -p${restore_password}'''
# do format, so we have {{ and }}
SINGLE_DB_RESTORE_TEMPLATE='''  mysql -h${{restore_host}} -P${{restore_port}} -u${{restore_user}} -p${{restore_password}} \\
    --database={db} -o
'''

def gen_backup_scripts(restore_path, backups, config, name=config.get('name')):
  os.makedirs(os.path.join(restore_path, 'scripts'), exist_ok=True)
  with open(os.path.join(restore_path, 'scripts', 'disable-bin-log.sql'), 'w') as f:
    f.write('SET SESSION SQL_LOG_BIN=0;\n')
  with open(os.path.join(restore_path, 'scripts', 'prepend'), 'w') as f:
    f.write('#!/bin/sh\necho -en "$@"\ncat -')
  for type in backups:
    for each in backups[type]:
      index = each['index']
      filename = '{index:03d}-{timestamp}{databases}{tables}.sh'.format(
        index=index, timestamp=each['timestamp'],
        databases='-'+' '.join(each['databases']) if each['databases'] else '',
        tables='-'+' '.join(each['tables']) if each['tables'] else '',
      )
      output = [
        '#!/bin/bash',
        'set -e',
        '# auto restore script (generate by mysql-rotate-backup)',
        '# restore parameters:',
        '#   name        = {}'.format(config.get('name')),
        '#   startBefore = {}'.format(config.get('startBefore')),
        '#   stopBefore  = {}'.format(config.get('stopBefore')),
        '#   databases   = {}'.format(config.get('databases')),
        '#   tables      = {}'.format(config.get('tables')),
        '# restore from',
        '#   ../../../{}'.format(each['relpath']),
        '#   timestamp     = {}'.format(each['timestamp']),
        '#   binlogn_start = {}'.format(each['binlog']),
        '#   binlogn_range = {}'.format(each['binlog_range']),
        '#   contain databases = {}'.format(each['databases']),
        '#   contain tables    = {}'.format(each['tables']),
        '#  errors:',
        '#    {}'.format(each['errors']) if each['errors'] else '',
      ]
      add_disable_bin_log = 'cat "${backup_file}" | gzip -d | bash prepend $(cat disable-bin-log.sql) | \\\n'
      if config.get('databases'):
        db_restore_string_list = [
            add_disable_bin_log + SINGLE_DB_RESTORE_TEMPLATE.format(db=db)
              for db in config.get('databases').split()
        ]
        db_restore_string = '\n'.join(['# databases: {}'.format(config['databases'])] + db_restore_string_list)
      else:
        db_restore_string = add_disable_bin_log + FULL_RESTORE_TEMPLATE
      error = each.get('errors', '')
      if each['binlogs']:
        binlog_str_list = [
          'mysqlbinlog \\',
        ]
        for eachbinlog in each['binlogs']:
          binlog_str_list.append(
            '  {binlog_dir}'.format(**config) +
              '/{eachbinlog} \\'.format(eachbinlog=eachbinlog)
          )
        binlog_str_list[-1] = binlog_str_list[-1][:-1] + '| \\'
        if config.get('databases'):
          _list = [ SINGLE_DB_RESTORE_TEMPLATE.format(db=db) for db in config.get('databases').split() ]
          _string = '\n'.join(_list)
        else:
          _string = FULL_RESTORE_TEMPLATE
        binlog_str_list.append(_string)
        binlog_restore_string = '\n'.join(binlog_str_list)
      else:
        binlog_restore_string = '# can not get binlogs for this bakcup, stop here'
      output.append(
        RESTORE_TEMPLATE.format(
          db_restore_string=db_restore_string,
          binlog_restore_string=binlog_restore_string,
          relpath=each['relpath'],
          binlog_range=each['binlog_range'],
          **config,
        )
      )
      with open(os.path.join(restore_path, 'scripts', filename), 'w') as f:
        f.write('\n'.join(output))

def do_restore(config=None, now=None, logfile=None):
  output = config['output']
  timestamp = now.strftime(TIMEFORMAT)
  if config.get('name'):
    restore_path = os.path.join(output, 'restore', config['name'])
  else:
    restore_path = os.path.join(output, 'restore', timestamp)
  stopBefore  = config.get('stopBefore', timestamp)   # (from) full backup should be earlier than this
  startBefore = config.get('startBefore', timestamp) # ( to ) binlog shoould be eariler than this
  stopBefore  = datetime.strptime(stopBefore, TIMEFORMAT)
  startBefore = datetime.strptime(startBefore, TIMEFORMAT)
  binlog_dir = config.get('binlog_dir')
  binlog_prefix = config.get('binlog_prefix')
  if (binlog_dir and binlog_prefix) and os.path.isdir(binlog_dir):
    binlogfiles = glob.glob(os.path.join(binlog_dir, binlog_prefix+'.??????'))
  else:
    binlogfiles = []
  binlogs = {}
  for each in binlogfiles:
    stat = os.stat(each)
    name = os.path.basename(each)
    ctime = datetime.fromtimestamp(stat.st_ctime)
    mtime = datetime.fromtimestamp(stat.st_mtime)
    binlogs[name] = {
      'ctime': ctime,
      'mtime': mtime,
      'size': humanize.naturalsize(stat.st_size)
    }
  goodBinlogName = list(filter(lambda key: binlogs[key]['ctime'] < stopBefore, binlogs.keys()))
  goodBinlogName.sort()
  goodBinLogs = {key:binlogs[key] for key in goodBinlogName}
  databases = config.get('databases')
  tables = config.get('tables')

  backups = {}
  for name in ['manual', 'auto']:
    backup_dir = os.path.join(output, 'backup', name)
    backups_raw = []
    for filename in find_files(backup_dir, '*.gz'):
      relpath = os.path.relpath(filename, config['output'])
      basename = os.path.basename(relpath)
      prefix, suffix = basename.split('|')
      _timestamp, _binlog = suffix.split('_')
      _binlog = _binlog.split('.gz')[0]
      if prefix == 'full':
        _table = None
        _database = None
      else:
        if '&' in prefix:
          _database, _table = prefix.split('&')
        else:
          if 'databases:' in prefix:
            _table = None
            _database = prefix
          elif 'tables:' in prefix:
            _database = None
            _table = prefix
        if _database:
          _database = _database.split('databases:')
          assert len(_database)==2 and _database[0] == '', 'error of file {}, {}, {}'.format(relpath, _database, prefix)
          _database = _database[1].strip().split()
        if _table:
          _table = _table.split('tables:')
          assert len(_table)==2 and _table[0] == '', 'error of file {}, {}, {}'.format(relpath, _table, prefix)
          _table = _table[1].strip().split()
      if not backup_match(databases, _database, tables, _table): # only have matched files
        continue
      if _binlog != 'nobinlog':
        _binlogs = list(filter(lambda name:name>=_binlog, goodBinlogName))
      else:
        _binlogs = []
      try:
        _binlog_range = check_binlog(_binlogs)
        _errors = ''
      except Exception as e:
        _binlog_range = []
        _errors = str(e)
      info = {
        'path':  filename,
        'relpath':  relpath,
        'dirname':  os.path.dirname(relpath),
        'basename': basename,
        'timestampStr': _timestamp,
        'timestamp': datetime.strptime(_timestamp, TIMEFORMAT),
        'binlog': _binlog,
        'binlog_range': _binlog_range,
        'binlogs': _binlogs,
        'errors': _errors,
        'tables': _table,
        'databases': _database,
      }
      backups_raw.append(info)
    backups_raw.sort(key=lambda _:_['timestamp'])
    backups[name] = list(filter(lambda _:_['timestamp']<startBefore, backups_raw))
  # generaate restore scripts
  os.makedirs(restore_path, exist_ok=True)
  ## binlogs.txt
  with open(os.path.join(restore_path, 'binlogs.txt'), 'w') as f:
    f.write(gen_binlog_info(binlogs, name=config.get('name')))
  ## backups.txt
  with open(os.path.join(restore_path, 'backups.txt'), 'w') as f:
    f.write(gen_backup_info(backups, config, name=config.get('name')))
  ## scripts
  gen_backup_scripts(restore_path, backups, config, name=config.get('name'))


def print_config(config):
  print('  configFile: {configFile}'.format(**config))
  databases = config.get('databases')
  tables = config.get('tables')
  _databases = ' databases: {databases}'.format(**config) if databases else ''
  _tables = ' tables: {tables}'.format(**config) if tables else ''
  print('  host: {host}, port: {port}, user: {user}'.format(**config)+_databases+_tables)
  print('  interval: {interval}, max-age:{max-age}'.format(**config))
  toPrints = [
    'max-year',
    'max-month',
    'max-week',
    'max-day',
    'max-hour',
    'max-minute',
  ]
  p = ['{key}:{value}'.format(key=_,value=config[_]) for _ in toPrints]
  print('  '+', '.join(p))
  toPrints = [
    'interval-year',
    'interval-month',
    'interval-week',
    'interval-day',
    'interval-hour',
    'interval-minute',
  ]
  p = ['{key}:{value}'.format(key=_,value=config[_]) for _ in toPrints]
  print('  '+', '.join(p))
  toPrints = [
    'start-month',
    'start-day-month',
    'start-day-week',
    'start-hour',
    'start-minute',
    'start-second',
  ]
  p = ['{key}:{value}'.format(key=_,value=config[_]) for _ in toPrints]
  print('  '+', '.join(p))

def do_unittest(config=None):
  now = datetime(year=2019, month=9, day=9, hour=13, minute=37, second=24)
  def gen_time(N, interval, fileinfo):
    for i in range(N):
      timestamp = now - i * interval
      timestampStr = timestamp.strftime(TIMEFORMAT)
      fileinfo.append({
        'filename': timestampStr,
        'timestampStr': timestampStr,
        'timestamp': timestamp,
      })
  def ppp(files):
    for each in files:
      print(each['filename'])
  fileinfo = []
  gen_time(120, process_time('1m'), fileinfo)
  gen_time(48,  process_time('1h'), fileinfo)
  gen_time(60,  process_time('1d'), fileinfo)
  gen_time(12,   process_time('1w'), fileinfo)
  gen_time(24,  process_time('1M'), fileinfo)
  gen_time(10,  process_time('1y'), fileinfo)
  fileinfo_set = {_['filename']:_ for _ in fileinfo}
  fileinfo = list(fileinfo_set.values())
  fileinfo.sort(key=lambda _:_['timestamp'])
  fileinfo.reverse()
  N = len(fileinfo)

  if 'test max-age':
    to_delete = []; to_reserve = copy.deepcopy(fileinfo);
    config['max-age'] = '1m'
    to_reserve, to_delete = delete_by_max_age(to_reserve, to_delete, now=now, config=config)
    assert len(to_reserve) == 1 and len(to_delete) == N-1

    to_delete = []; to_reserve = copy.deepcopy(fileinfo);
    config['max-age'] = '10m'
    to_reserve, to_delete = delete_by_max_age(to_reserve, to_delete, now=now, config=config)
    assert len(to_reserve) == 10 and len(to_delete) == N-10

    to_delete = []; to_reserve = copy.deepcopy(fileinfo);
    config['max-age'] = '90m'
    to_reserve, to_delete = delete_by_max_age(to_reserve, to_delete, now=now, config=config)
    assert len(to_reserve) == 90 and len(to_delete) == N-90

    to_delete = []; to_reserve = copy.deepcopy(fileinfo);
    config['max-age'] = '2h'
    to_reserve, to_delete = delete_by_max_age(to_reserve, to_delete, now=now, config=config)
    assert len(to_reserve) == 120 and len(to_delete) == N-120

    to_delete = []; to_reserve = copy.deepcopy(fileinfo);
    config['max-age'] = '3h'
    to_reserve, to_delete = delete_by_max_age(to_reserve, to_delete, now=now, config=config)
    assert len(to_reserve) == 121 and len(to_delete) == N-121

    to_delete = []; to_reserve = copy.deepcopy(fileinfo);
    config['max-age'] = '5h'
    to_reserve, to_delete = delete_by_max_age(to_reserve, to_delete, now=now, config=config)
    assert len(to_reserve) == 123 and len(to_delete) == N-123

    to_delete = []; to_reserve = copy.deepcopy(fileinfo);
    config['max-age'] = '1d'
    to_reserve, to_delete = delete_by_max_age(to_reserve, to_delete, now=now, config=config)
    assert len(to_reserve) == 142 and len(to_delete) == N-142

    to_delete = []; to_reserve = copy.deepcopy(fileinfo);
    config['max-age'] = '2d'
    to_reserve, to_delete = delete_by_max_age(to_reserve, to_delete, now=now, config=config)
    assert len(to_reserve) == 166 and len(to_delete) == N-166

    to_delete = []; to_reserve = copy.deepcopy(fileinfo);
    config['max-age'] = '3d'
    to_reserve, to_delete = delete_by_max_age(to_reserve, to_delete, now=now, config=config)
    assert len(to_reserve) == 167 and len(to_delete) == N-167

    to_delete = []; to_reserve = copy.deepcopy(fileinfo);
    config['max-age'] = '7d'
    to_reserve, to_delete = delete_by_max_age(to_reserve, to_delete, now=now, config=config)
    assert len(to_reserve) == 171 and len(to_delete) == N-171

    to_delete = []; to_reserve = copy.deepcopy(fileinfo);
    config['max-age'] = '14d'
    to_reserve, to_delete = delete_by_max_age(to_reserve, to_delete, now=now, config=config)
    assert len(to_reserve) == 178 and len(to_delete) == N-178
  if 'test interval':
    to_delete = []; to_reserve = copy.deepcopy(fileinfo);
    config['interval'] = '1m'
    to_reserve, to_delete = delete_by_min_interval(to_reserve, to_delete, now=now, config=config)
    assert len(to_reserve) == N and len(to_delete) == 0

    to_delete = []; to_reserve = copy.deepcopy(fileinfo);
    config['interval'] = '2m'
    to_reserve, to_delete = delete_by_min_interval(to_reserve, to_delete, now=now, config=config)
    assert len(to_reserve) == N - 60 and len(to_delete) == 60

    to_delete = []; to_reserve = copy.deepcopy(fileinfo);
    config['interval'] = '3m'
    to_reserve, to_delete = delete_by_min_interval(to_reserve, to_delete, now=now, config=config)
    assert len(to_reserve) == N - 80 and len(to_delete) == 80

    to_delete = []; to_reserve = copy.deepcopy(fileinfo);
    config['interval'] = '10m'
    to_reserve, to_delete = delete_by_min_interval(to_reserve, to_delete, now=now, config=config)
    assert len(to_reserve) == N - 108 and len(to_delete) == 108

    to_delete = []; to_reserve = copy.deepcopy(fileinfo);
    config['interval'] = '1h'
    to_reserve, to_delete = delete_by_min_interval(to_reserve, to_delete, now=now, config=config)
    assert len(to_reserve) == N - 118 and len(to_delete) == 118

    to_delete = []; to_reserve = copy.deepcopy(fileinfo);
    config['interval'] = '2h'
    to_reserve, to_delete = delete_by_min_interval(to_reserve, to_delete, now=now, config=config)
    assert len(to_reserve) == N - 142 and len(to_delete) == 142

    to_delete = []; to_reserve = copy.deepcopy(fileinfo);
    config['interval'] = '3h'
    to_reserve, to_delete = delete_by_min_interval(to_reserve, to_delete, now=now, config=config)
    assert len(to_reserve) == N - 150 and len(to_delete) == 150

    to_delete = []; to_reserve = copy.deepcopy(fileinfo);
    config['interval'] = '8h'
    to_reserve, to_delete = delete_by_min_interval(to_reserve, to_delete, now=now, config=config)
    assert len(to_reserve) == N - 160 and len(to_delete) == 160

    to_delete = []; to_reserve = copy.deepcopy(fileinfo);
    config['interval'] = '1d'
    to_reserve, to_delete = delete_by_min_interval(to_reserve, to_delete, now=now, config=config)
    assert len(to_reserve) == N - 164 and len(to_delete) == 164

    to_delete = []; to_reserve = copy.deepcopy(fileinfo);
    config['interval'] = '2d'
    to_reserve, to_delete = delete_by_min_interval(to_reserve, to_delete, now=now, config=config)
    assert len(to_reserve) == N - 195 and len(to_delete) == 195

    to_delete = []; to_reserve = copy.deepcopy(fileinfo);
    config['interval'] = '3d'
    to_reserve, to_delete = delete_by_min_interval(to_reserve, to_delete, now=now, config=config)
    assert len(to_reserve) == N - 205 and len(to_delete) == 205

    to_delete = []; to_reserve = copy.deepcopy(fileinfo);
    config['interval'] = '1w'
    to_reserve, to_delete = delete_by_min_interval(to_reserve, to_delete, now=now, config=config)
    assert len(to_reserve) == N - 216 and len(to_delete) == 216

    to_delete = []; to_reserve = copy.deepcopy(fileinfo);
    config['interval'] = '2w'
    to_reserve, to_delete = delete_by_min_interval(to_reserve, to_delete, now=now, config=config)
    assert len(to_reserve) == N - 222 and len(to_delete) == 222

    to_delete = []; to_reserve = copy.deepcopy(fileinfo);
    config['interval'] = '3w'
    to_reserve, to_delete = delete_by_min_interval(to_reserve, to_delete, now=now, config=config)
    assert len(to_reserve) == N - 224 and len(to_delete) == 224

    to_delete = []; to_reserve = copy.deepcopy(fileinfo);
    config['interval'] = '1M'
    to_reserve, to_delete = delete_by_min_interval(to_reserve, to_delete, now=now, config=config)
    assert len(to_reserve) == N - 225 and len(to_delete) == 225

    to_delete = []; to_reserve = copy.deepcopy(fileinfo);
    config['interval'] = '3M'
    to_reserve, to_delete = delete_by_min_interval(to_reserve, to_delete, now=now, config=config)
    assert len(to_reserve) == N - 241 and len(to_delete) == 241

    to_delete = []; to_reserve = copy.deepcopy(fileinfo);
    config['interval'] = '1y'
    to_reserve, to_delete = delete_by_min_interval(to_reserve, to_delete, now=now, config=config)
    assert len(to_reserve) == N - 247 and len(to_delete) == 247

    to_delete = []; to_reserve = copy.deepcopy(fileinfo);
    config['interval'] = '3y'
    to_reserve, to_delete = delete_by_min_interval(to_reserve, to_delete, now=now, config=config)
    assert len(to_reserve) == 4 and len(to_delete) == N - 4
  if 'test level':
    to_delete = []; to_reserve = copy.deepcopy(fileinfo);
    config['max-year']  = 5
    config['max-month']  = 12
    config['max-week']   = 8
    config['max-day']    = 7
    config['max-hour']   = 24
    config['max-minute'] = 60
    config['interval-year']   = 2
    config['interval-month']  = 2
    config['interval-week']   = 2
    config['interval-day']    = 2
    config['interval-hour']   = 3
    config['interval-minute'] = 20
    config['start-month']     = 1 # 1~12
    config['start-day-month'] = 1 # 1~28, can be negitave
    config['start-day-week']  = 0 # 1~7
    config['start-hour']      = 0 # 0~59
    config['start-minute']    = 0 # 0~59
    config['start-second']    = 0 # 0~59
    to_reserve, to_delete = delete_in_levels(to_reserve, to_delete, config=config)


  print(len(to_delete))
  import ipdb
  ipdb.set_trace()


  import ipdb
  ipdb.set_trace()

if __name__ == '__main__':
  args = parser.parse_args()
  if not args.command:
    parser.print_help()
    sys.exit(0)

  now = datetime.now()
  if args.command == 'unittest': # do unittest
    do_unittest(config=config)
    sys.exit(0)
  configFile = args.config
  configFileExists = True
  if configFile is None: # use default config file
    configFile = DEFAULT_CONFIG_FILE
    if not os.path.exists(configFile):
      configFileExists = False
  else:
    if not os.path.exists(configFile):
      print('config file {} not exists!'.format(configFile))
      sys.exit(1)
  if configFileExists:
    config['configFile'] = configFile
    with open(configFile) as f: # update with config file
      config_from_file = yaml.safe_load(f)
      if config_from_file is not None:
        for key in config_from_file:
          if config_from_file[key] is not None:
            config[key] = config_from_file[key]
  else:
    config['configFile'] = None

  # update with input args
  argsDict = args.__dict__
  for key in argsDict:
    if argsDict[key] is not None:
      config[key] = argsDict[key]
  if args.command == 'init-config':
    print(DEFAULT_CONFIG)
    sys.exit(0)
  check_config(config)
  if args.command == 'config':
    print('# final combined config from configFile:{} and command args'.format(config['configFile']))
    print(json.dumps(config, indent=2))
    sys.exit(0)

  ## do jogs
  if args.command not in ['backup', 'status', 'restore']:
    print('command should be backup, status or restore')
    sys.exit(1)
  output = config['output']
  os.makedirs(output, exist_ok=True)
  os.makedirs(os.path.join(output, 'restore'), exist_ok=True)
  os.makedirs(os.path.join(output, 'backup'), exist_ok=True)
  os.makedirs(os.path.join(output, 'backup', 'auto'), exist_ok=True)
  os.makedirs(os.path.join(output, 'backup', 'manual'), exist_ok=True)
  logfile = os.path.join(output, 'log.log')

  if args.command == 'backup':
    do_backup(config=config, now=now, logfile=logfile)
  elif args.command == 'status':
    do_status(config=config, now=now, logfile=logfile)
  elif args.command == 'restore':
    do_restore(config=config, now=now, logfile=logfile)


